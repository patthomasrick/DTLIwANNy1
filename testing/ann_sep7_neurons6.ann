FANN_FLO_2.1
num_layers=3
learning_rate=0.700000
connection_rate=1.000000
network_type=0
learning_momentum=0.000000
training_algorithm=2
train_error_function=1
train_stop_function=0
cascade_output_change_fraction=0.010000
quickprop_decay=-0.000100
quickprop_mu=1.750000
rprop_increase_factor=1.200000
rprop_decrease_factor=0.500000
rprop_delta_min=0.000000
rprop_delta_max=50.000000
rprop_delta_zero=0.100000
cascade_output_stagnation_epochs=12
cascade_candidate_change_fraction=0.010000
cascade_candidate_stagnation_epochs=12
cascade_max_out_epochs=150
cascade_max_cand_epochs=150
cascade_num_candidate_groups=2
bit_fail_limit=3.49999999999999977796e-01
cascade_candidate_limit=1.00000000000000000000e+03
cascade_weight_multiplier=4.00000000000000022204e-01
cascade_activation_functions_count=10
cascade_activation_functions=3 5 7 8 10 11 14 15 16 17 
cascade_activation_steepnesses_count=4
cascade_activation_steepnesses=2.50000000000000000000e-01 5.00000000000000000000e-01 7.50000000000000000000e-01 1.00000000000000000000e+00 
layer_sizes=8 7 2 
scale_included=0
neurons (num_inputs, activation_function, activation_steepness)=(0, 0, 0.00000000000000000000e+00) (0, 0, 0.00000000000000000000e+00) (0, 0, 0.00000000000000000000e+00) (0, 0, 0.00000000000000000000e+00) (0, 0, 0.00000000000000000000e+00) (0, 0, 0.00000000000000000000e+00) (0, 0, 0.00000000000000000000e+00) (0, 0, 0.00000000000000000000e+00) (8, 4, 5.00000000000000000000e-01) (8, 4, 5.00000000000000000000e-01) (8, 4, 5.00000000000000000000e-01) (8, 4, 5.00000000000000000000e-01) (8, 4, 5.00000000000000000000e-01) (8, 4, 5.00000000000000000000e-01) (0, 0, 0.00000000000000000000e+00) (7, 6, 5.00000000000000000000e-01) (0, 6, 0.00000000000000000000e+00) 
connections (connected_to_neuron, weight)=(0, -1.85747287725232474997e+01) (1, -2.19349620245094500959e+00) (2, 5.42768387598107437952e+00) (3, 5.51440134892543176726e-01) (4, -7.12311255800525700010e+00) (5, 1.43181149828061560214e+01) (6, 6.27116402256171880936e+01) (7, -3.02405668549235473463e+00) (0, -1.63519379844851471262e+01) (1, -1.79774161690180434015e+00) (2, 5.49200125839126318539e+00) (3, -3.53542405688837768629e-01) (4, -7.45112889427790303642e+00) (5, 9.08192165003026019576e+00) (6, 4.02211522718907019680e+01) (7, -1.49518464981578591555e+00) (0, -4.86579222714048853504e+01) (1, -1.37723206485079541217e+00) (2, 1.69667090679391350250e+01) (3, 3.34995523939319816975e+00) (4, -7.33249841702974136659e-01) (5, -9.09144420963259491941e+00) (6, -6.32752573352592762035e+01) (7, 1.49902046056601756518e+00) (0, 3.13018266717529769494e+01) (1, -1.40821815336227729176e+00) (2, 9.46184922889185031636e+00) (3, -4.16528135614109062956e-01) (4, -3.04557382219262322565e+01) (5, -4.88375260557909207648e+00) (6, -2.22067797926455234858e+01) (7, 1.54439152407912527742e+00) (0, -1.59635792321721048381e+01) (1, -5.63397310986149424394e+00) (2, 3.28407049878207724092e+00) (3, -4.00051929541474116103e-01) (4, -7.54588519260725387738e+00) (5, 1.00602210840041728090e+01) (6, 4.00701242985671015617e+01) (7, -9.75802612348209419402e-01) (0, -4.84701406410578101713e+01) (1, -1.39655690916637964527e+00) (2, 1.47605382380635390405e+01) (3, 4.01412585423335244172e+00) (4, -1.08758013981597279063e+00) (5, -8.83387168212945361745e+00) (6, -4.47749541627021869772e+01) (7, 1.51900980945986829873e+00) (8, -2.82286693971835767059e+01) (9, -5.25385411350023456833e+00) (10, -1.08344726295237734348e+01) (11, -1.96609953051322712270e+01) (12, -5.18149360963496086185e+00) (13, -1.87991163936741578766e+01) (14, 1.27030494063503329727e+01) 

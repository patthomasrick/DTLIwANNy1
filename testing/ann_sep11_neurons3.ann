FANN_FLO_2.1
num_layers=3
learning_rate=0.700000
connection_rate=1.000000
network_type=0
learning_momentum=0.000000
training_algorithm=2
train_error_function=1
train_stop_function=0
cascade_output_change_fraction=0.010000
quickprop_decay=-0.000100
quickprop_mu=1.750000
rprop_increase_factor=1.200000
rprop_decrease_factor=0.500000
rprop_delta_min=0.000000
rprop_delta_max=50.000000
rprop_delta_zero=0.100000
cascade_output_stagnation_epochs=12
cascade_candidate_change_fraction=0.010000
cascade_candidate_stagnation_epochs=12
cascade_max_out_epochs=150
cascade_max_cand_epochs=150
cascade_num_candidate_groups=2
bit_fail_limit=3.49999999999999977796e-01
cascade_candidate_limit=1.00000000000000000000e+03
cascade_weight_multiplier=4.00000000000000022204e-01
cascade_activation_functions_count=10
cascade_activation_functions=3 5 7 8 10 11 14 15 16 17 
cascade_activation_steepnesses_count=4
cascade_activation_steepnesses=2.50000000000000000000e-01 5.00000000000000000000e-01 7.50000000000000000000e-01 1.00000000000000000000e+00 
layer_sizes=12 4 2 
scale_included=0
neurons (num_inputs, activation_function, activation_steepness)=(0, 0, 0.00000000000000000000e+00) (0, 0, 0.00000000000000000000e+00) (0, 0, 0.00000000000000000000e+00) (0, 0, 0.00000000000000000000e+00) (0, 0, 0.00000000000000000000e+00) (0, 0, 0.00000000000000000000e+00) (0, 0, 0.00000000000000000000e+00) (0, 0, 0.00000000000000000000e+00) (0, 0, 0.00000000000000000000e+00) (0, 0, 0.00000000000000000000e+00) (0, 0, 0.00000000000000000000e+00) (0, 0, 0.00000000000000000000e+00) (12, 4, 5.00000000000000000000e-01) (12, 4, 5.00000000000000000000e-01) (12, 4, 5.00000000000000000000e-01) (0, 0, 0.00000000000000000000e+00) (4, 6, 5.00000000000000000000e-01) (0, 6, 0.00000000000000000000e+00) 
connections (connected_to_neuron, weight)=(0, -8.40806721083096277880e+01) (1, -1.04247590273554937568e+01) (2, -6.48395740018923638637e-01) (3, 4.56392344487481560478e-01) (4, 5.22835762679340820824e+00) (5, 2.95235617520905258004e+00) (6, 2.92760849417723206800e-02) (7, -1.86340156414577950983e+00) (8, 1.76695113593626951243e+00) (9, 1.31991966635170632216e+00) (10, 7.69442859947516240027e+01) (11, 1.68612957894425141703e+00) (0, 2.12076390463327051350e+01) (1, -4.08710480701645195722e+00) (2, 8.02470366643268318008e-01) (3, -8.44488767146567176347e-01) (4, 6.31203775236763586776e+00) (5, 5.98458350554099638430e+00) (6, -2.05017864558607110226e+00) (7, -8.52622968521418833632e+00) (8, -1.68260064505881423713e+01) (9, -5.75297513400335469580e-02) (10, 3.64678426368031161076e+00) (11, 2.68660772735440023240e+00) (0, -3.45517405949537170784e+00) (1, -3.62013133888577121056e+00) (2, -1.34231078597320818702e+00) (3, 3.75160718831475448187e-01) (4, 4.16868723694355924891e+00) (5, 6.85179004910737576850e-01) (6, -7.54354671751023353465e+00) (7, -7.95675105821829209418e+00) (8, -2.52428028513075997097e+00) (9, 1.57040721078671587918e+01) (10, 6.58542500499188747654e+01) (11, 7.86392040368391276317e-01) (12, -1.36804420599986134022e+01) (13, -9.90931077438094831678e+00) (14, -1.77827969981467290950e+01) (15, 9.55040255966007300970e+00) 

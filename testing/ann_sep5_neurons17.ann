FANN_FLO_2.1
num_layers=3
learning_rate=0.700000
connection_rate=1.000000
network_type=0
learning_momentum=0.000000
training_algorithm=2
train_error_function=1
train_stop_function=0
cascade_output_change_fraction=0.010000
quickprop_decay=-0.000100
quickprop_mu=1.750000
rprop_increase_factor=1.200000
rprop_decrease_factor=0.500000
rprop_delta_min=0.000000
rprop_delta_max=50.000000
rprop_delta_zero=0.100000
cascade_output_stagnation_epochs=12
cascade_candidate_change_fraction=0.010000
cascade_candidate_stagnation_epochs=12
cascade_max_out_epochs=150
cascade_max_cand_epochs=150
cascade_num_candidate_groups=2
bit_fail_limit=3.49999999999999977796e-01
cascade_candidate_limit=1.00000000000000000000e+03
cascade_weight_multiplier=4.00000000000000022204e-01
cascade_activation_functions_count=10
cascade_activation_functions=3 5 7 8 10 11 14 15 16 17 
cascade_activation_steepnesses_count=4
cascade_activation_steepnesses=2.50000000000000000000e-01 5.00000000000000000000e-01 7.50000000000000000000e-01 1.00000000000000000000e+00 
layer_sizes=6 18 2 
scale_included=0
neurons (num_inputs, activation_function, activation_steepness)=(0, 0, 0.00000000000000000000e+00) (0, 0, 0.00000000000000000000e+00) (0, 0, 0.00000000000000000000e+00) (0, 0, 0.00000000000000000000e+00) (0, 0, 0.00000000000000000000e+00) (0, 0, 0.00000000000000000000e+00) (6, 4, 5.00000000000000000000e-01) (6, 4, 5.00000000000000000000e-01) (6, 4, 5.00000000000000000000e-01) (6, 4, 5.00000000000000000000e-01) (6, 4, 5.00000000000000000000e-01) (6, 4, 5.00000000000000000000e-01) (6, 4, 5.00000000000000000000e-01) (6, 4, 5.00000000000000000000e-01) (6, 4, 5.00000000000000000000e-01) (6, 4, 5.00000000000000000000e-01) (6, 4, 5.00000000000000000000e-01) (6, 4, 5.00000000000000000000e-01) (6, 4, 5.00000000000000000000e-01) (6, 4, 5.00000000000000000000e-01) (6, 4, 5.00000000000000000000e-01) (6, 4, 5.00000000000000000000e-01) (6, 4, 5.00000000000000000000e-01) (0, 0, 0.00000000000000000000e+00) (18, 6, 5.00000000000000000000e-01) (0, 6, 0.00000000000000000000e+00) 
connections (connected_to_neuron, weight)=(0, -3.68624157300022972095e+01) (1, 9.43416996280995512336e+00) (2, -1.26785397779746276115e+01) (3, 1.78997350945951474444e+01) (4, 2.30842691695217716585e+01) (5, 2.16089674425194555596e+00) (0, -1.53013709595424867160e+01) (1, 7.95003710039635080875e+00) (2, 4.51777289821197414188e-01) (3, -5.11951634767679930604e+00) (4, 5.82630316974526873963e+00) (5, 2.95589602365251824523e+00) (0, 1.47786545721336074166e+03) (1, -1.26402142073743675610e+03) (2, 2.96615748159079828383e+02) (3, -1.31502800772375348970e+03) (4, -1.47868171710813589925e+03) (5, 5.64139366872986464108e-02) (0, 1.46790843526765706883e+03) (1, -1.26377112920993181433e+03) (2, 3.11003951575953806241e+02) (3, -1.32249186692198577475e+03) (4, -1.47868195819492643750e+03) (5, 1.55659651289069139724e-01) (0, -3.64306047578796210473e+01) (1, 5.43272980088727219794e+00) (2, -2.67869696065676832220e+00) (3, -2.42902408718180939928e+00) (4, 3.98909927199259186636e+01) (5, 1.89108319451821849100e+00) (0, -3.07095996709421648063e+01) (1, 3.82057905495889293945e+00) (2, -3.12235398832068566932e+00) (3, -3.34334043870258446063e+00) (4, 7.32036001049708602295e+01) (5, 1.54635024680276411679e+00) (0, -8.31006498034808771536e+01) (1, 1.75981386507475567882e+00) (2, -6.72747672711064392814e+00) (3, -6.14328330417707419997e+01) (4, 4.47379762888585730707e+02) (5, 4.28488094816624709438e+00) (0, 1.46787177530115604895e+03) (1, -1.26369137400520844494e+03) (2, 3.11003951575953806241e+02) (3, -1.32249215492200869448e+03) (4, -1.47868195819492643750e+03) (5, 1.45888099606894527938e-01) (0, 1.47868465704858431309e+03) (1, -1.26389404391151833806e+03) (2, 3.08051033436057821291e+02) (3, -1.34065942567808338026e+03) (4, -1.47867909236035006870e+03) (5, -4.05795365269622843041e-01) (0, -1.29503366157577848305e-01) (1, 6.75768434718034249897e+00) (2, -1.27577487528425881358e+00) (3, -1.92020263381808078407e+01) (4, -1.37057213371596077423e+02) (5, 2.33193713647418610080e+00) (0, 1.46791334395734725149e+03) (1, -1.26377112920993181433e+03) (2, 3.11003951575953806241e+02) (3, -1.32249186692198577475e+03) (4, -1.47868195819492643750e+03) (5, 1.54571404585922139718e-01) (0, 1.46789154508136516597e+03) (1, -1.26377112920993181433e+03) (2, 3.11003951575953806241e+02) (3, -1.32249186692198577475e+03) (4, -1.47868195819492643750e+03) (5, 1.59613387300377995714e-01) (0, 1.46776532410141771834e+03) (1, -1.26351037776109569677e+03) (2, 3.11003788375921033094e+02) (3, -1.32251565948527172623e+03) (4, -1.47868195819492643750e+03) (5, 1.30854900087980091650e-01) (0, -8.67702743821749464814e+01) (1, 7.06771701257540208019e+00) (2, 4.31143480668675227463e+00) (3, 3.86087594698433456131e-01) (4, -3.56918477771674176324e+01) (5, 1.10394104003387916180e+00) (0, 1.49041773160997240666e+03) (1, -1.28300689709306243458e+03) (2, 3.44725398529875121767e+02) (3, -1.39983979703227487335e+03) (4, -1.47714746110081159713e+03) (5, 4.32806946225682498408e+00) (0, 1.47788094806207686815e+03) (1, -1.26409795449148759872e+03) (2, 2.96615959378195100271e+02) (3, -1.31564316242185600458e+03) (4, -1.47868171710813589925e+03) (5, 1.15897771946634309259e-01) (0, -8.38083405396726845993e+01) (1, -2.48752534779800207332e+01) (2, -7.07080218345246969136e+00) (3, 1.57582540593481024160e+01) (4, -4.95196921800334507680e+02) (5, 1.99679408215708988905e+01) (6, -3.62900666752094238632e+01) (7, 1.98356318852499882155e+01) (8, 8.50000000000000000000e+02) (9, 7.50000000000000000000e+02) (10, -5.46969290400309304800e+01) (11, -4.03128993698391866474e+01) (12, 1.86666705260491653462e+02) (13, 7.50000000000000000000e+02) (14, 9.50000000000000000000e+02) (15, -8.92763564478282916070e+01) (16, 7.50000000000000000000e+02) (17, 7.50000000000000000000e+02) (18, 7.50000000000000000000e+02) (19, -7.47388601945827645068e+02) (20, -6.00000000000000000000e+02) (21, 8.50000000000000000000e+02) (22, 2.65584245024564715720e+01) (23, 7.87371081242314385662e+00) 

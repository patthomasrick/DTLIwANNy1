FANN_FLO_2.1
num_layers=3
learning_rate=0.700000
connection_rate=1.000000
network_type=0
learning_momentum=0.000000
training_algorithm=2
train_error_function=1
train_stop_function=0
cascade_output_change_fraction=0.010000
quickprop_decay=-0.000100
quickprop_mu=1.750000
rprop_increase_factor=1.200000
rprop_decrease_factor=0.500000
rprop_delta_min=0.000000
rprop_delta_max=50.000000
rprop_delta_zero=0.100000
cascade_output_stagnation_epochs=12
cascade_candidate_change_fraction=0.010000
cascade_candidate_stagnation_epochs=12
cascade_max_out_epochs=150
cascade_max_cand_epochs=150
cascade_num_candidate_groups=2
bit_fail_limit=3.49999999999999977796e-01
cascade_candidate_limit=1.00000000000000000000e+03
cascade_weight_multiplier=4.00000000000000022204e-01
cascade_activation_functions_count=10
cascade_activation_functions=3 5 7 8 10 11 14 15 16 17 
cascade_activation_steepnesses_count=4
cascade_activation_steepnesses=2.50000000000000000000e-01 5.00000000000000000000e-01 7.50000000000000000000e-01 1.00000000000000000000e+00 
layer_sizes=11 5 2 
scale_included=0
neurons (num_inputs, activation_function, activation_steepness)=(0, 0, 0.00000000000000000000e+00) (0, 0, 0.00000000000000000000e+00) (0, 0, 0.00000000000000000000e+00) (0, 0, 0.00000000000000000000e+00) (0, 0, 0.00000000000000000000e+00) (0, 0, 0.00000000000000000000e+00) (0, 0, 0.00000000000000000000e+00) (0, 0, 0.00000000000000000000e+00) (0, 0, 0.00000000000000000000e+00) (0, 0, 0.00000000000000000000e+00) (0, 0, 0.00000000000000000000e+00) (11, 4, 5.00000000000000000000e-01) (11, 4, 5.00000000000000000000e-01) (11, 4, 5.00000000000000000000e-01) (11, 4, 5.00000000000000000000e-01) (0, 0, 0.00000000000000000000e+00) (5, 6, 5.00000000000000000000e-01) (0, 6, 0.00000000000000000000e+00) 
connections (connected_to_neuron, weight)=(0, 3.35606528603459164373e+00) (1, -1.12375397058325159350e+01) (2, 1.03431213865053717171e+00) (3, 4.41762183196013957787e-01) (4, 3.82576450556856695329e+00) (5, -3.68969003942132500740e+00) (6, -1.27696107369308613499e+01) (7, -1.41529363622777015763e-01) (8, 6.38839124329739238561e+00) (9, 1.76125930927695350192e+02) (10, 3.73329409648885990336e+00) (0, -8.69530413010830756093e+01) (1, -6.23646689271762699747e+00) (2, 9.79631732805393951047e-01) (3, 4.71974751618894305949e+00) (4, 7.51612654585221218184e+00) (5, -1.81433661396116674425e+00) (6, -8.40565725408825414888e+00) (7, -2.65294870649924208195e+00) (8, 5.64164291461626721258e-01) (9, 2.01282189393012203027e+02) (10, 3.85456728000413262336e+00) (0, -9.14585717787017102864e+01) (1, -4.95845697796745366048e+00) (2, 9.16914404276765404589e-01) (3, 5.40134033442705874251e+00) (4, 7.73504352809815909353e+00) (5, 1.82019181561770326816e+00) (6, -5.86640116028427360817e-01) (7, -7.48989535848801457618e-01) (8, -2.53935732126047319923e+00) (9, 1.60854882866547114872e+02) (10, 4.23801568921232707510e+00) (0, 6.04478089138243745992e+00) (1, -1.67801721456210017891e+00) (2, 2.77585543048619542983e-01) (3, 1.74433493983863363574e+00) (4, 2.55575024026639496810e+00) (5, -3.35825345075895409508e+00) (6, -8.86528883907924836194e+00) (7, -7.18553800403033293698e-01) (8, 4.38873953468949551393e+00) (9, 6.22099964632578874557e+01) (10, 1.94120680082851615467e+00) (11, -8.44545636926194731586e+00) (12, -6.43674277625715376416e+01) (13, 1.50220782569201389833e+01) (14, -1.30878143416672791943e+01) (15, 1.12220644810727421259e+01) 

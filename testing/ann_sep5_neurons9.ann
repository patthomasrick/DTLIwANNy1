FANN_FLO_2.1
num_layers=3
learning_rate=0.700000
connection_rate=1.000000
network_type=0
learning_momentum=0.000000
training_algorithm=2
train_error_function=1
train_stop_function=0
cascade_output_change_fraction=0.010000
quickprop_decay=-0.000100
quickprop_mu=1.750000
rprop_increase_factor=1.200000
rprop_decrease_factor=0.500000
rprop_delta_min=0.000000
rprop_delta_max=50.000000
rprop_delta_zero=0.100000
cascade_output_stagnation_epochs=12
cascade_candidate_change_fraction=0.010000
cascade_candidate_stagnation_epochs=12
cascade_max_out_epochs=150
cascade_max_cand_epochs=150
cascade_num_candidate_groups=2
bit_fail_limit=3.49999999999999977796e-01
cascade_candidate_limit=1.00000000000000000000e+03
cascade_weight_multiplier=4.00000000000000022204e-01
cascade_activation_functions_count=10
cascade_activation_functions=3 5 7 8 10 11 14 15 16 17 
cascade_activation_steepnesses_count=4
cascade_activation_steepnesses=2.50000000000000000000e-01 5.00000000000000000000e-01 7.50000000000000000000e-01 1.00000000000000000000e+00 
layer_sizes=6 10 2 
scale_included=0
neurons (num_inputs, activation_function, activation_steepness)=(0, 0, 0.00000000000000000000e+00) (0, 0, 0.00000000000000000000e+00) (0, 0, 0.00000000000000000000e+00) (0, 0, 0.00000000000000000000e+00) (0, 0, 0.00000000000000000000e+00) (0, 0, 0.00000000000000000000e+00) (6, 4, 5.00000000000000000000e-01) (6, 4, 5.00000000000000000000e-01) (6, 4, 5.00000000000000000000e-01) (6, 4, 5.00000000000000000000e-01) (6, 4, 5.00000000000000000000e-01) (6, 4, 5.00000000000000000000e-01) (6, 4, 5.00000000000000000000e-01) (6, 4, 5.00000000000000000000e-01) (6, 4, 5.00000000000000000000e-01) (0, 0, 0.00000000000000000000e+00) (10, 6, 5.00000000000000000000e-01) (0, 6, 0.00000000000000000000e+00) 
connections (connected_to_neuron, weight)=(0, -3.36305544071624922253e+01) (1, 5.32474354152506723636e+00) (2, -4.78703007123177481930e+00) (3, 1.14072614901031887946e+00) (4, 2.73774939741406797111e+02) (5, 1.51961053368277987374e+00) (0, 4.13537355227389813450e+00) (1, 6.57394573233947010493e+00) (2, -4.61881338652376616238e+00) (3, -1.44013146438721175713e+01) (4, -2.17578041556933555967e+02) (5, 2.03875399999621897962e+00) (0, -2.18907044862774524674e+00) (1, 6.90218332642397047749e+00) (2, -3.96096209676210797568e+00) (3, -1.16162860686270814625e+01) (4, -1.26115900717190683622e+02) (5, 1.45278871326340874859e+00) (0, 1.75190384734430153912e+02) (1, -1.21589193234016420320e+03) (2, 1.48393119024576003540e+03) (3, -1.03088089690670335585e+03) (4, -1.11874395708587644549e+03) (5, -5.69901288884551107117e+01) (0, -4.25583134780900795846e+00) (1, -8.72119122947559222148e-01) (2, -4.28542015190025527005e+00) (3, 3.34369359799371723696e+00) (4, -3.33669022511347179716e+02) (5, 3.20567569808053276503e+00) (0, -3.19836723750023210755e+01) (1, 6.52846241645633096340e+00) (2, 5.40089829091593109922e+00) (3, -1.43033979836016040821e+01) (4, 3.34263861261247939183e+02) (5, 2.63488203598468917477e-01) (0, -4.69392548349770848404e+01) (1, 9.72426152866492010673e+00) (2, 1.46458506268012422780e+00) (3, 1.92576687417162717431e+00) (4, 9.21242618467465774756e+01) (5, 2.17443577518970865281e-01) (0, -1.36437121412817887744e+01) (1, -1.41328780052899969633e+03) (2, 1.49835337209415843063e+03) (3, -1.04390781831287654313e+03) (4, -1.43251904203706567387e+03) (5, 1.51649231701633246949e+00) (0, -4.62334033634822219483e+02) (1, -1.23096795856508288125e+03) (2, 1.48700393142037614780e+03) (3, -1.03121224438676449608e+03) (4, -1.25548594450069549566e+03) (5, -3.49809347919811264305e+00) (6, -5.30137490784999272364e+01) (7, -1.97352023694129172782e+02) (8, -2.88011427948219527195e+01) (9, 8.41912367777889130593e+02) (10, 8.10880374708420532670e+01) (11, 6.50321473403998311369e+01) (12, -4.85634313057098196964e+01) (13, -1.08102658139396294246e+03) (14, 2.44318234931858739856e+02) (15, 5.47433252017562121949e+00) 

FANN_FLO_2.1
num_layers=3
learning_rate=0.700000
connection_rate=1.000000
network_type=0
learning_momentum=0.000000
training_algorithm=2
train_error_function=1
train_stop_function=0
cascade_output_change_fraction=0.010000
quickprop_decay=-0.000100
quickprop_mu=1.750000
rprop_increase_factor=1.200000
rprop_decrease_factor=0.500000
rprop_delta_min=0.000000
rprop_delta_max=50.000000
rprop_delta_zero=0.100000
cascade_output_stagnation_epochs=12
cascade_candidate_change_fraction=0.010000
cascade_candidate_stagnation_epochs=12
cascade_max_out_epochs=150
cascade_max_cand_epochs=150
cascade_num_candidate_groups=2
bit_fail_limit=3.49999999999999977796e-01
cascade_candidate_limit=1.00000000000000000000e+03
cascade_weight_multiplier=4.00000000000000022204e-01
cascade_activation_functions_count=10
cascade_activation_functions=3 5 7 8 10 11 14 15 16 17 
cascade_activation_steepnesses_count=4
cascade_activation_steepnesses=2.50000000000000000000e-01 5.00000000000000000000e-01 7.50000000000000000000e-01 1.00000000000000000000e+00 
layer_sizes=3 16 2 
scale_included=0
neurons (num_inputs, activation_function, activation_steepness)=(0, 0, 0.00000000000000000000e+00) (0, 0, 0.00000000000000000000e+00) (0, 0, 0.00000000000000000000e+00) (3, 4, 5.00000000000000000000e-01) (3, 4, 5.00000000000000000000e-01) (3, 4, 5.00000000000000000000e-01) (3, 4, 5.00000000000000000000e-01) (3, 4, 5.00000000000000000000e-01) (3, 4, 5.00000000000000000000e-01) (3, 4, 5.00000000000000000000e-01) (3, 4, 5.00000000000000000000e-01) (3, 4, 5.00000000000000000000e-01) (3, 4, 5.00000000000000000000e-01) (3, 4, 5.00000000000000000000e-01) (3, 4, 5.00000000000000000000e-01) (3, 4, 5.00000000000000000000e-01) (3, 4, 5.00000000000000000000e-01) (3, 4, 5.00000000000000000000e-01) (0, 0, 0.00000000000000000000e+00) (16, 6, 5.00000000000000000000e-01) (0, 6, 0.00000000000000000000e+00) 
connections (connected_to_neuron, weight)=(0, 9.73689488107754499424e+01) (1, 1.50000000000000000000e+03) (2, -2.65923092461497603267e+01) (0, 9.36622341444492292339e+01) (1, -1.03693483570565808805e+03) (2, -2.88790553265096825442e+01) (0, -9.09433923157669532600e+00) (1, -1.38838983699901518776e+02) (2, 4.66731929741734230532e+00) (0, 1.68096968834131721415e+00) (1, -3.08491885770008821055e+02) (2, 2.96954525524937018588e+00) (0, 1.60119834575282879996e+01) (1, -1.35419907789948979371e+03) (2, 8.73491804955724937543e+00) (0, -1.12017540659395837821e+01) (1, -1.50000000000000000000e+03) (2, 1.37089882018073279113e+01) (0, 1.33281789094221636560e+02) (1, 1.17986036935457195796e+03) (2, -2.59856181230180247610e+01) (0, 1.40675619191525555607e+02) (1, 1.24590639170186886986e+03) (2, -2.75902463132200992391e+01) (0, 1.40772522115376091278e+02) (1, 1.24601434049957629213e+03) (2, -2.76060494064131169978e+01) (0, -1.33970237382274994786e+01) (1, -7.70896386408611760999e+02) (2, 6.71159911831156374973e+00) (0, 3.10205103954514882503e+00) (1, -8.42266715971393637119e+02) (2, 3.64078526065182428439e+00) (0, -3.25127835468997758994e+01) (1, -1.32395290094309643791e+02) (2, 7.91822925676853461852e+00) (0, 9.67376389701695371670e-01) (1, -1.00952642332115669888e+03) (2, 1.72817505219211291490e+01) (0, 1.22686217558815812367e+02) (1, 1.49999993999999765038e+03) (2, -2.64718454478463769419e+01) (0, 1.25395223174872057825e+02) (1, 1.49999993999999765038e+03) (2, -2.68976431135660476457e+01) (3, -1.95621323919520371248e+01) (4, 5.33823281619220040284e+01) (5, 1.01761861027648109257e+02) (6, 3.54694351530357465663e+01) (7, -2.67020862632375930446e+01) (8, 6.90750268213338642909e+01) (9, -4.67127772714793909614e+00) (10, -3.90006990403264053313e+00) (11, -3.94989827080018462979e+00) (12, -1.05638929755255688292e+02) (13, -9.49063693848881371196e+01) (14, -4.39264970748217180585e+01) (15, -3.63027100324140974408e+01) (16, -4.78857651504541070864e+00) (17, -3.45100825093435847890e+00) (18, 4.35895634311277557771e+00) 
